{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOb/k1p6rRUEuIuhEzOm39N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makxxic/AI-for-SDevWK6/blob/main/AI4SDevWK6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"techsash/waste-classification-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEL9IIrhFh1k",
        "outputId": "41caf32a-ef73-42b8-b30f-947324262ba6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'waste-classification-data' dataset.\n",
            "Path to dataset files: /kaggle/input/waste-classification-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d0fb7ac",
        "outputId": "986b78a7-d10e-49b0-cc92-794960e9fe4d"
      },
      "source": [
        "import os\n",
        "\n",
        "# The 'path' variable holds the base directory of the downloaded dataset.\n",
        "# Its current value is: '/kaggle/input/waste-classification-data'\n",
        "print(f\"Inspecting the base dataset directory: {path}\")\n",
        "\n",
        "# List all contents (files and subdirectories) at the root of the downloaded dataset\n",
        "print(\"Contents of the downloaded dataset:\")\n",
        "for item in os.listdir(path):\n",
        "    full_path = os.path.join(path, item)\n",
        "    if os.path.isdir(full_path):\n",
        "        print(f\"- {item}/ (Directory)\")\n",
        "        # Optionally, list contents of one level deeper if it seems relevant\n",
        "        # if item == 'DATASET': # Example: if there's a 'DATASET' folder\n",
        "        #     print(f\"  Contents of {item}/\")\n",
        "        #     for sub_item in os.listdir(full_path):\n",
        "        #         print(f\"  - {sub_item}\")\n",
        "    else:\n",
        "        print(f\"- {item} (File)\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting the base dataset directory: /kaggle/input/waste-classification-data\n",
            "Contents of the downloaded dataset:\n",
            "- DATASET/ (Directory)\n",
            "- dataset/ (Directory)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZbooTLuELKIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39343739-f366-401e-f929-4036c6af4610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22564 images belonging to 2 classes.\n",
            "Found 2513 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 199ms/step - accuracy: 0.7828 - loss: 0.5123 - val_accuracy: 0.8818 - val_loss: 0.3330\n",
            "Epoch 2/5\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.8483 - loss: 0.3564 - val_accuracy: 0.8707 - val_loss: 0.3397\n",
            "Epoch 3/5\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.8775 - loss: 0.2955 - val_accuracy: 0.8822 - val_loss: 0.2994\n",
            "Epoch 4/5\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.9016 - loss: 0.2387 - val_accuracy: 0.8818 - val_loss: 0.3325\n",
            "Epoch 5/5\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9313 - loss: 0.1739 - val_accuracy: 0.8886 - val_loss: 0.3296\n",
            "Saved artifact at '/tmp/tmpavngznfz'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134504692083280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692084432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692084816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692083856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692083472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692085008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692081936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134504692083664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Load Dataset\n",
        "# The 'path' variable is set in the first cell (gEL9IIrhFh1k) by kagglehub.dataset_download\n",
        "# Based on inspection, TRAIN and TEST folders are likely inside a 'DATASET' subdirectory.\n",
        "train_dir = os.path.join(path, 'DATASET', 'TRAIN') # Corrected path\n",
        "val_dir = os.path.join(path, 'DATASET', 'TEST')  # Corrected path\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_data = datagen.flow_from_directory(train_dir, target_size=(128,128), batch_size=32, class_mode='binary')\n",
        "val_data = datagen.flow_from_directory(val_dir, target_size=(128,128), batch_size=32, class_mode='binary')\n",
        "\n",
        "# 2. Build Lightweight CNN Model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=5)\n",
        "\n",
        "# 3. Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save model\n",
        "with open('recycle_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0e35b2a",
        "outputId": "e891d752-44d5-4ad7-b825-c4ae8a40909d"
      },
      "source": [
        "import os\n",
        "\n",
        "# The train_dir is already defined from the previous cells.\n",
        "# In your current setup, it is: '/kaggle/input/waste-classification-data/TRAIN'\n",
        "print(f\"Path to the main training directory: {train_dir}\")\n",
        "\n",
        "# List all items (subdirectories and files) within the training directory\n",
        "print(\"Contents of the training directory:\")\n",
        "for item in os.listdir(train_dir):\n",
        "    print(f\"- {item}\")\n",
        "\n",
        "# Example: If there's a subdirectory named 'plastic' inside 'TRAIN',\n",
        "# you can access its path like this:\n",
        "# plastic_dir = os.path.join(train_dir, 'plastic')\n",
        "# print(f\"Path to the 'plastic' subdirectory: {plastic_dir}\")\n",
        "\n",
        "# To see the contents of a specific subdirectory (e.g., 'plastic' if it exists):\n",
        "# if 'plastic' in os.listdir(train_dir):\n",
        "#     print(\"\\nContents of the 'plastic' subdirectory:\")\n",
        "#     for item in os.listdir(os.path.join(train_dir, 'plastic')):\n",
        "#         print(f\"  - {item}\")\n",
        "# else:\n",
        "#     print(\"\\n'plastic' subdirectory not found in training data.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the main training directory: /kaggle/input/waste-classification-data/DATASET/TRAIN\n",
            "Contents of the training directory:\n",
            "- R\n",
            "- O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044f670a",
        "outputId": "c3192811-ab78-4f7e-9023-751456f7c0e3"
      },
      "source": [
        "# Install the ai_edge_litert package\n",
        "!pip install ai-edge-litert\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ai-edge-litert\n",
            "  Downloading ai_edge_litert-2.0.3-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert)\n",
            "  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.15.0)\n",
            "Downloading ai_edge_litert-2.0.3-cp312-cp312-manylinux_2_27_x86_64.whl (91.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: backports.strenum, ai-edge-litert\n",
            "Successfully installed ai-edge-litert-2.0.3 backports.strenum-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48841efc",
        "outputId": "f559592d-e4ea-4d5b-e62e-809862929d7c"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import ai_edge_litert.interpreter as tflite  # New import for LiteRT\n",
        "import os\n",
        "\n",
        "# Instantiate the LiteRT interpreter with the TFLite model\n",
        "interpreter = tflite.Interpreter(model_path=\"recycle_classifier.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Use the same dummy image as before if it exists, or create one.\n",
        "if not os.path.exists(\"plastic_bottle.jpg\"):\n",
        "    dummy_image = Image.new('RGB', (128, 128), color = 'red')\n",
        "    dummy_image.save(\"plastic_bottle.jpg\")\n",
        "\n",
        "image = Image.open(\"plastic_bottle.jpg\").resize((128,128))\n",
        "input_data = np.expand_dims(np.array(image, dtype=np.float32)/255.0, axis=0)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Prediction using LiteRT:\", \"Recyclable\" if output_data[0]>0.5 else \"Non-Recyclable\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction using LiteRT: Recyclable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow.lite as tflite # Changed import from tflite_runtime.interpreter\n",
        "\n",
        "interpreter = tflite.Interpreter(model_path=\"recycle_classifier.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Ensure 'plastic_bottle.jpg' exists or provide a placeholder image.\n",
        "# For demonstration, let's create a dummy image if it doesn't exist.\n",
        "import os\n",
        "if not os.path.exists(\"plastic_bottle.jpg\"):\n",
        "    dummy_image = Image.new('RGB', (128, 128), color = 'red')\n",
        "    dummy_image.save(\"plastic_bottle.jpg\")\n",
        "\n",
        "image = Image.open(\"plastic_bottle.jpg\").resize((128,128))\n",
        "input_data = np.expand_dims(np.array(image, dtype=np.float32)/255.0, axis=0)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Prediction:\", \"Recyclable\" if output_data[0]>0.5 else \"Non-Recyclable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK3sFRABGaju",
        "outputId": "b406f103-77c5-41e4-907f-90eced0c3ef2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Recyclable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    }
  ]
}